{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "client = AsyncOpenAI()\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all texts from text folder\n",
    "texts = []\n",
    "for file in os.listdir(\"text\"):\n",
    "    with open(os.path.join(\"text\", file), \"r\") as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "I have a raw transcript of a 911 call with some words and sentences mistranscribed. I want to turn this into an OpenAI-compatible messages JSON array so that I can use this to fine tune an LLM on being a 911 operator. Clean up the transcript slightly, rewriting (but not pmmitting) lines that make sense for the operator and caller to say. Use the \"assistant\" role for operator and \"user\" role for caller. Give me the messages array.\n",
    "\n",
    "The messages array should be formatted like this:\n",
    "\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"9-1-1, what's your emergency?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"<caller message>\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "The output should always start with \"9-1-1, what's your emergency?\".\n",
    "\n",
    "The user will input a transcript where each new line may have been said by the operator or the caller.\n",
    "\"\"\"\n",
    "async def process_text(text):\n",
    "    result = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "    return result.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 30 seconds before next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [01:59<09:58, 119.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 30 seconds before next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [03:25<06:39, 99.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 30 seconds before next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [04:54<04:44, 94.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 30 seconds before next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [06:20<03:02, 91.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 30 seconds before next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [08:26<00:00, 84.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text 64: Expecting property name enclosed in double quotes: line 553 column 1 (char 18102)\n",
      "Error processing text 88: Expecting property name enclosed in double quotes: line 588 column 12 (char 18163)\n",
      "Error processing text 143: Expecting property name enclosed in double quotes: line 461 column 1 (char 18456)\n",
      "Error processing text 179: Unterminated string starting at: line 669 column 24 (char 18245)\n",
      "Error processing text 282: Unterminated string starting at: line 585 column 24 (char 18173)\n",
      "Error processing text 507: Unterminated string starting at: line 589 column 24 (char 18274)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "processed_texts_saved = []\n",
    "async def process_batch(batch):\n",
    "    tasks = [process_text(text) for text in batch]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def process_all_texts():\n",
    "    global processed_texts_saved\n",
    "    batch_size = 100\n",
    "    delay = 30  # seconds\n",
    "    processed_texts = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        results = await process_batch(batch)\n",
    "        \n",
    "        # Store processed results\n",
    "        processed_texts.extend(results)\n",
    "        \n",
    "        # Save processed results to a file after each batch\n",
    "        with open('processed_texts.json', 'w') as f:\n",
    "            json.dump(processed_texts, f)\n",
    "        \n",
    "        if i + batch_size < len(texts):\n",
    "            print(f\"Waiting {delay} seconds before next batch...\")\n",
    "            await asyncio.sleep(delay)\n",
    "            \n",
    "    processed_texts_saved = processed_texts\n",
    "\n",
    "    # After all batches are processed, save to individual files\n",
    "    if not os.path.exists('processed'):\n",
    "        os.makedirs('processed')\n",
    "    \n",
    "    for idx, processed_text in enumerate(processed_texts):\n",
    "        try:\n",
    "            with open(f'processed/text_{idx}.json', 'w') as f:\n",
    "                json.dump(json.loads(processed_text), f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text {idx}: {e}\")\n",
    "\n",
    "# Run the async function\n",
    "await process_all_texts()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calhacks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
